Table Of Contents:

1. Introduction to Machine Learning Optimizers
2. Types of Machine Learning Optimizers 
3. Gradient Descent: The First Optimizer
4. Stochastic Gradient Descent (SGD)
5. Momentum Based Gradient Descent 
6. Nesterov Accelerated Gradient (NAG)
7. Adaptive Gradient Algorithms (AdaGrad) 
8. RMSprop: Root Mean Square Propagation
9. Adaptive Moment Estimation (Adam)
10. AdaDelta
11. Natural Gradient Descent
12. Krylov Subspace Methods
13. Conjugate Gradient Method
14. Limited Memory Broyden-Fletcher Goldfarb Shanno (L-BFGS) 
15. Nonlinear Conjugate Gradient (NCG)
16. Levenberg-Marquardt Algorithm 
17. Gauss-Newton Algorithm
18. Broyden's Method
19. Quasi-Newton methods 
20. Trust-Region Methods
21. Dogleg method
22. Truncated Newton (TNewton) method
23. Proximal Gradient Method
24. Alternating Direction Method of Multipliers (ADMM)
25. Linear Search 
26. Binary Search
27. Golden Section Search
28. Backtracking Line Search 
29. Armijo-Goldstein Condition 
30. Subsampling Optimization
31. Acceleration techniques in Deep Learning Optimizers
32. Ensemble of Optimizers
33. Conclusion